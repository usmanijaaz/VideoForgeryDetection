{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFJKLdGbR-WA",
        "outputId": "4a006618-9219-4598-a8a4-17e5346af4cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aXgGKPZ9Siv0",
        "outputId": "1888fca1-5d37-4049-d27c-75da632d1402"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Real Videos : 169 Fake Videos : 626\n"
          ]
        }
      ],
      "source": [
        "#loading Data\n",
        "import glob\n",
        "\n",
        "video_files =  glob.glob('/content/drive/My Drive/DeepFake/original_sequences/youtube/c23/videos/*.mp4')\n",
        "video_files +=  glob.glob('/content/drive/My Drive/DeepFake/original_sequences/actors/c23/videos/*.mp4')\n",
        "\n",
        "#print('ReaL:', video_files, '\\n')\n",
        "\n",
        "fake_vids = glob.glob('/content/drive/My Drive/DeepFake/manipulated_sequences/NeuralTextures/c23/videos/*.mp4')\n",
        "fake_vids += glob.glob('/content/drive/My Drive/DeepFake/manipulated_sequences/FaceSwap/c23/videos/*.mp4')\n",
        "fake_vids += glob.glob('/content/drive/My Drive/DeepFake/manipulated_sequences/FaceShifter/c23/videos/*.mp4')\n",
        "fake_vids += glob.glob('/content/drive/My Drive/DeepFake/manipulated_sequences/Face2Face/c23/videos/*.mp4')\n",
        "fake_vids += glob.glob('/content/drive/My Drive/DeepFake/manipulated_sequences/Deepfakes/c23/videos/*.mp4')\n",
        "fake_vids += glob.glob('/content/drive/My Drive/DeepFake/manipulated_sequences/DeepFakeDetection/c23/videos/*.mp4')\n",
        "\n",
        "#print('Fake:' , fake_vids, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tKX3JTBaYfwj"
      },
      "outputs": [],
      "source": [
        "#extracting frames\n",
        "import cv2\n",
        "def getFrames(vid_path):\n",
        "  vidObj = cv2.VideoCapture(vid_path)\n",
        "  cont = 1\n",
        "  while cont:\n",
        "    cont, image = vidObj.read()\n",
        "    if cont == 1:\n",
        "      yield image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8BTgch2cltV"
      },
      "outputs": [],
      "source": [
        "!mkdir '/content/drive/My Drive/Real_Face_videos_after_cropping'\n",
        "!mkdir '/content/drive/My Drive/Fake_Face_videos_after_cropping'\n",
        "!mkdir '/content/drive/My Drive/CSVFile'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDjtoUSJ3nF1",
        "outputId": "7c33b496-9112-46b4-ce82-75fa1edf13e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dlib in /usr/local/lib/python3.7/dist-packages (19.18.0)\n",
            "Collecting face_recognition\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/95/f6c9330f54ab07bfa032bf3715c12455a381083125d8880c43cbe76bb3d0/face_recognition-1.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (19.18.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/3b/4fd8c534f6c0d1b80ce0973d01331525538045084c73c153ee6df20224cf/face_recognition_models-0.3.0.tar.gz (100.1MB)\n",
            "\u001b[K     |████████████████████████████████| 100.2MB 74kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_recognition) (1.19.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566184 sha256=504dc591e403d615ade67867b3a18c2eb0bc4b9fd15632c964347fc03eb3e4b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/99/18/59c6c8f01e39810415c0e63f5bede7d83dfb0ffc039865465f\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install dlib\n",
        "import dlib\n",
        "!pip3 install face_recognition\n",
        "import face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2NEGIjbrk7I",
        "outputId": "07fedf13-ef16-47ba-c046-960d459abf8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "from tqdm.autonotebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ycKbMyv3yEaA"
      },
      "outputs": [],
      "source": [
        "def createVideosFace(videos,outputPath):\n",
        "  for each in tqdm(videos):\n",
        "    vid_path = os.path.join(outputPath,each.split('/')[-1])\n",
        "    exists = glob.glob(vid_path)\n",
        "    if len(exists)!= 0:\n",
        "      vid_path = os.path.join(outputPath,each.split('/')[-4]+each.split('/')[-1])\n",
        "    videoFrames = []\n",
        "    booll = 0\n",
        "    Faces = []\n",
        "    vidFrames2 = []\n",
        "    out = cv2.VideoWriter(vid_path,cv2.VideoWriter_fourcc('M','J','P','G'), 30, (112,112))\n",
        "    for i,frame in enumerate(getFrames(each)):\n",
        "      if i<=150:\n",
        "        videoFrames.append(frame)\n",
        "        face_locations = face_recognition.face_locations(frame, model=\"cnn\")\n",
        "        for i,face in enumerate(face_locations):\n",
        "            if(len(face) != 0): \n",
        "              top,right,bottom,left = face         \n",
        "            try:\n",
        "              out.write(cv2.resize(videoFrames[i][top:bottom,left:right,:],(112,112)))\n",
        "            except:\n",
        "              pass\n",
        "    videoFrames = []\n",
        "    try:\n",
        "      del top,right,bottom,left\n",
        "    except:\n",
        "      pass\n",
        "    out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iJriyTz3E8q"
      },
      "outputs": [],
      "source": [
        "createVideosFace(video_files,'/content/drive/My Drive/Real_Face_videos_after_cropping')\n",
        "createVideosFace(fake_vids,'/content/drive/My Drive/Fake_Face_videos_after_cropping')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_shD-skXN2A",
        "outputId": "2a7d325d-3867-4744-affd-de05f96a02a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "totalVids = video_files + fake_vids\n",
        "len(totalVids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0A9H2N5cREs",
        "outputId": "0078d86c-87b8-48af-83c9-4ac050b4d9c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "86\n",
            "85\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "vids =  glob.glob('/content/drive/My Drive/Real_Face_videos_after_cropping/*.mp4')\n",
        "vids1 =  glob.glob('/content/drive/My Drive/Fake_Face_videos_after_cropping/*.mp4')\n",
        "\n",
        "csvRows = [[]]\n",
        "csvRows.append([\"File Name\", \"Label\"])\n",
        "for each in vids:\n",
        "  csvRows.append([each.split('/')[-1],'Real'])\n",
        "for each in vids1:\n",
        "  csvRows.append([each.split('/')[-1],'Fake'])\n",
        "\n",
        "with open('/content/drive/My Drive/CSVFile/videofileslabels.csv', 'w') as file:\n",
        "  writer = csv.writer(file)\n",
        "  writer.writerows(csvRows)\n",
        "   \n",
        "vids += vids1\n",
        "count =0\n",
        "print(len(vids))\n",
        "for i in vids:\n",
        "    count+=1\n",
        "    vid = cv2.VideoCapture(i)\n",
        "    try:\n",
        "      if not vid.isOpened():   #corrupted video\n",
        "        vids.remove(i)\n",
        "        raise NameError(\"Phew!! Video is Corrupted !!\")\n",
        "    except:\n",
        "      pass\n",
        "    else:\n",
        "      pass\n",
        "      #print('Valid Video !!!')\n",
        "\n",
        "print(len(vids))\n",
        "\n",
        "#shuffle the dataset to avoid training on same data\n",
        "\n",
        "random.shuffle(vids)\n",
        "random.shuffle(vids)\n",
        "for video_file in vids:\n",
        "  cap = cv2.VideoCapture(video_file)\n",
        "  if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))<120): #removing videos less than 120fps(4sec)\n",
        "    vids.remove(video_file)\n",
        "    continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uZUmY6kbVwZ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "class MakeVideosDataset(Dataset):\n",
        "  def __init__(self,videonames,labels,frameslength = 40,transform = None):\n",
        "    self.videonames = videonames\n",
        "    self.labels = labels\n",
        "    self.count = frameslength\n",
        "    self.trans = transform\n",
        "  def __len__(self):\n",
        "    return len(self.videonames)\n",
        "    \n",
        "  def __getitem__(self,index):\n",
        "    video = self.videonames[index]\n",
        "    label = self.labels[index]\n",
        "    frames=[]\n",
        "    filename = video.split('/')[-1]\n",
        "\n",
        "    label = self.labels[index]\n",
        "    if(label == 'Fake'):\n",
        "      label = 0\n",
        "    if(label == 'Real'):\n",
        "      label = 1\n",
        "    for i,frame in enumerate(getFrames(video)):\n",
        "      frames.append(self.trans(frame))\n",
        "      if(len(frames) == self.count):\n",
        "        break\n",
        "    frames = torch.stack(frames)\n",
        "    frames = frames[:self.count]\n",
        "  \n",
        "    return frames,label\n",
        "\n",
        "def im_plot(tensor):\n",
        "  image = tensor.cpu().numpy().transpose(1,2,0)\n",
        "  b,g,r = cv2.split(image)\n",
        "  image = cv2.merge((r,g,b))\n",
        "  image = image*[0.22803, 0.22145, 0.216989] +  [0.43216, 0.394666, 0.37645]\n",
        "  image = image*255.0\n",
        "  plt.imshow(image.astype(int))\n",
        "  plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYfzYgxkr0vm",
        "outputId": "aa094cd6-5b70-4bfd-b0ba-fe5e09aeb733"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 3, 112, 112])"
            ]
          },
          "execution_count": 29,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/CSVFile/videofileslabels.csv')\n",
        "filenames = df['File Name']\n",
        "labels = df['Label']\n",
        "#print(labels)\n",
        "X_train = vids[:int(0.8*len(vids))]\n",
        "Y_test = vids[int(0.8*len(vids)):]\n",
        "\n",
        "im_size = 112\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                                        transforms.ToPILImage(),\n",
        "                                        transforms.Resize((im_size,im_size)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean,std)])\n",
        "\n",
        "data = MakeVideosDataset(X_train,labels,frameslength = 10,transform = train_transforms)\n",
        "vdata = MakeVideosDataset(Y_test,labels,frameslength = 10,transform = train_transforms)\n",
        "\n",
        "t_loader = DataLoader(dataset = data,batch_size = 4,shuffle = True,num_workers = 4)\n",
        "v_loader = DataLoader(dataset = vdata,batch_size = 4,shuffle = True,num_workers = 4)\n",
        "\n",
        "#image,label = data[21]\n",
        "#im_plot(image[1,:,:,:])\n",
        "data[0][0].shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPyQSqsjx5m_"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes,latent_dim= 2048, lstm_layers=1 , hidden_dim = 2048, bidirectional = False):\n",
        "        super(Model, self).__init__()\n",
        "        model = models.resnext50_32x4d(pretrained = True) #Residual Network CNN\n",
        "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
        "        self.lstm = nn.LSTM(latent_dim,hidden_dim, lstm_layers,  bidirectional)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dp = nn.Dropout(0.4)\n",
        "        self.linear1 = nn.Linear(2048,num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    def forward(self, x):\n",
        "        batch_size,seq_length, c, h, w = x.shape\n",
        "        x = x.view(batch_size * seq_length, c, h, w)\n",
        "        fmap = self.model(x)\n",
        "        x = self.avgpool(fmap)\n",
        "        x = x.view(batch_size,seq_length,2048)\n",
        "        x_lstm,_ = self.lstm(x,None)\n",
        "        return fmap,self.dp(self.linear1(torch.mean(x_lstm,dim = 1)))\n",
        "        #return x_lstm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EidVU48-ikMK",
        "outputId": "782f3e05-f58f-4ba0-d9bb-c0a88c95b465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[[[9.9543e-02, 4.4869e-02, 9.1683e-01, 2.0721e-01],\n",
            "          [0.0000e+00, 8.9498e-01, 1.2581e+00, 6.4056e-01],\n",
            "          [0.0000e+00, 5.0756e-01, 1.3630e-01, 1.2502e+00],\n",
            "          [0.0000e+00, 4.3099e-01, 2.6047e-02, 1.4396e+00]],\n",
            "\n",
            "         [[0.0000e+00, 6.3883e-01, 9.9331e-01, 4.7795e-01],\n",
            "          [0.0000e+00, 7.8276e-01, 7.9952e-01, 9.3080e-01],\n",
            "          [0.0000e+00, 6.4783e-01, 1.1627e+00, 1.3978e+00],\n",
            "          [0.0000e+00, 4.0856e-01, 1.0398e-01, 3.4952e-01]],\n",
            "\n",
            "         [[3.7643e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [7.4328e-01, 6.6736e-01, 9.4043e-01, 0.0000e+00],\n",
            "          [1.3036e+00, 6.4928e-03, 5.4710e-01, 0.0000e+00],\n",
            "          [1.0907e+00, 1.1043e+00, 1.4767e+00, 9.3433e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 1.5648e-01, 0.0000e+00],\n",
            "          [0.0000e+00, 5.7000e-01, 6.7901e-01, 0.0000e+00],\n",
            "          [4.8722e-01, 6.0042e-01, 1.2955e+00, 2.2123e-01],\n",
            "          [2.1556e-01, 1.0206e+00, 5.4502e-01, 5.4633e-01]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 8.5250e-01, 8.1622e-01],\n",
            "          [0.0000e+00, 0.0000e+00, 3.5726e-01, 7.0313e-01],\n",
            "          [3.2089e-01, 0.0000e+00, 1.2409e+00, 9.9928e-01],\n",
            "          [4.7424e-01, 2.5850e-01, 8.4149e-01, 4.3619e-01]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 1.0487e+00, 0.0000e+00, 2.6982e-01],\n",
            "          [1.4026e+00, 1.2075e+00, 1.0150e+00, 5.7099e-01],\n",
            "          [7.9136e-01, 1.5659e+00, 1.5395e+00, 7.2952e-01]]],\n",
            "\n",
            "\n",
            "        [[[1.2665e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [6.4383e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 1.8846e+00, 8.8346e-01, 1.8082e+00]],\n",
            "\n",
            "         [[5.8078e-01, 0.0000e+00, 5.3279e-01, 0.0000e+00],\n",
            "          [3.0983e-01, 4.8319e-02, 1.6901e+00, 8.5091e-01],\n",
            "          [2.1899e+00, 6.8852e-01, 2.0083e+00, 8.0960e-01],\n",
            "          [1.7056e+00, 1.8018e+00, 2.5074e+00, 1.7632e+00]],\n",
            "\n",
            "         [[1.7311e+00, 6.8126e-01, 0.0000e+00, 0.0000e+00],\n",
            "          [9.9020e-01, 8.7900e-01, 3.1525e-01, 4.6752e-01],\n",
            "          [1.1674e+00, 4.8858e-01, 6.1515e-01, 0.0000e+00],\n",
            "          [3.2446e-01, 6.1784e-01, 5.6614e-01, 1.7054e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.0450e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [1.2120e+00, 0.0000e+00, 5.2667e-01, 1.4243e-01],\n",
            "          [2.5128e-01, 0.0000e+00, 7.1589e-01, 3.0879e-01],\n",
            "          [1.2869e+00, 1.9397e+00, 1.7331e+00, 7.4404e-01]],\n",
            "\n",
            "         [[2.5596e+00, 4.1287e-01, 7.2725e-01, 3.7760e-01],\n",
            "          [1.2873e+00, 2.0855e-02, 1.1357e+00, 1.6325e+00],\n",
            "          [2.3552e+00, 1.0881e+00, 1.6631e+00, 1.5037e+00],\n",
            "          [0.0000e+00, 3.8660e+00, 4.4839e+00, 6.2691e+00]],\n",
            "\n",
            "         [[2.5428e+00, 1.3460e+00, 1.2864e+00, 6.0028e-01],\n",
            "          [2.6956e+00, 5.3355e-01, 2.0940e-01, 2.4714e-01],\n",
            "          [2.9218e+00, 0.0000e+00, 3.2910e-01, 5.8290e-02],\n",
            "          [1.9307e+00, 1.6407e+00, 1.6214e+00, 1.2885e+00]]],\n",
            "\n",
            "\n",
            "        [[[4.2517e-02, 0.0000e+00, 6.3449e-01, 2.3783e-01],\n",
            "          [0.0000e+00, 8.8658e-01, 1.3047e+00, 1.0737e+00],\n",
            "          [0.0000e+00, 1.4693e-01, 7.4810e-02, 1.3078e+00],\n",
            "          [0.0000e+00, 4.2115e-01, 5.1695e-02, 1.4692e+00]],\n",
            "\n",
            "         [[0.0000e+00, 5.0283e-01, 8.3720e-01, 1.7978e-01],\n",
            "          [0.0000e+00, 6.1222e-01, 6.8386e-01, 7.0492e-01],\n",
            "          [0.0000e+00, 4.9296e-02, 9.8600e-01, 1.0480e+00],\n",
            "          [0.0000e+00, 1.0945e-01, 3.0164e-02, 1.8216e-01]],\n",
            "\n",
            "         [[3.1118e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [6.2051e-01, 5.1395e-02, 6.0963e-01, 0.0000e+00],\n",
            "          [1.0192e+00, 0.0000e+00, 3.6739e-01, 0.0000e+00],\n",
            "          [7.8747e-01, 8.0485e-01, 1.2151e+00, 9.0070e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 2.8220e-01, 0.0000e+00],\n",
            "          [0.0000e+00, 3.1193e-01, 6.3766e-01, 0.0000e+00],\n",
            "          [6.7160e-01, 8.4785e-01, 1.8405e+00, 1.7605e-01],\n",
            "          [5.4678e-01, 1.0006e+00, 7.2583e-01, 2.9484e-01]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 1.2143e+00, 1.2719e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 2.7232e-01, 1.1852e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 3.1772e-01, 5.7421e-01],\n",
            "          [0.0000e+00, 0.0000e+00, 8.2714e-02, 2.7940e-01]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 2.7580e-01, 0.0000e+00, 0.0000e+00],\n",
            "          [1.1063e+00, 6.8145e-01, 2.5817e-01, 4.1847e-02],\n",
            "          [7.0057e-03, 0.0000e+00, 5.5117e-01, 1.5407e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[8.1684e-01, 2.8050e-01, 8.9651e-01, 2.4129e-01],\n",
            "          [8.7540e-01, 1.4437e-01, 9.2354e-01, 0.0000e+00],\n",
            "          [1.4829e+00, 1.0506e+00, 1.4724e+00, 6.2975e-01],\n",
            "          [8.1617e-01, 1.0876e+00, 1.5189e+00, 1.0206e+00]],\n",
            "\n",
            "         [[6.5576e-01, 7.8910e-01, 1.5322e+00, 3.8157e-01],\n",
            "          [1.5612e+00, 8.4751e-01, 2.4426e+00, 6.5645e-01],\n",
            "          [1.1041e+00, 2.1732e+00, 3.3065e+00, 2.0269e+00],\n",
            "          [1.6503e+00, 2.0096e+00, 2.4295e+00, 9.5672e-01]],\n",
            "\n",
            "         [[3.4282e-01, 0.0000e+00, 1.6030e-01, 0.0000e+00],\n",
            "          [1.6518e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [2.6941e-01, 0.0000e+00, 3.7948e-01, 0.0000e+00],\n",
            "          [2.8440e-01, 0.0000e+00, 7.9025e-02, 0.0000e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 2.0116e-01, 1.2245e-01],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 4.5240e-01, 0.0000e+00]],\n",
            "\n",
            "         [[2.1591e+00, 3.2007e+00, 3.1742e+00, 2.0581e+00],\n",
            "          [1.7751e+00, 2.6070e+00, 3.7785e+00, 1.8473e+00],\n",
            "          [1.8424e+00, 1.7740e+00, 3.3966e+00, 2.2206e+00],\n",
            "          [1.5117e+00, 1.8028e+00, 2.1590e+00, 4.9609e-01]],\n",
            "\n",
            "         [[1.6034e+00, 2.1011e+00, 1.0207e+00, 4.8754e-01],\n",
            "          [2.2152e+00, 8.4121e-01, 1.5589e+00, 3.1197e-01],\n",
            "          [3.8595e+00, 2.8968e+00, 2.2845e+00, 1.4047e+00],\n",
            "          [2.9861e+00, 1.7325e+00, 2.2065e+00, 6.5363e-02]]],\n",
            "\n",
            "\n",
            "        [[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 1.1474e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 2.9617e-01, 0.0000e+00, 4.4732e-01],\n",
            "          [0.0000e+00, 1.0950e+00, 4.5940e-01, 1.3955e+00]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 1.4535e-01, 2.0630e-01, 0.0000e+00],\n",
            "          [2.5110e-01, 1.2075e+00, 1.3375e+00, 1.0797e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 1.3671e-01, 7.7793e-02]],\n",
            "\n",
            "         [[0.0000e+00, 3.3898e-01, 7.9090e-01, 2.0666e-01],\n",
            "          [8.0715e-01, 1.1035e+00, 2.0119e-02, 0.0000e+00],\n",
            "          [7.4356e-01, 1.3827e+00, 1.2391e+00, 7.6286e-01],\n",
            "          [1.6400e+00, 1.3031e+00, 0.0000e+00, 1.3954e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6165e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 4.3919e-01, 8.0434e-01],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 4.2670e-01, 2.8881e-01],\n",
            "          [0.0000e+00, 0.0000e+00, 1.5791e-01, 2.2043e-01],\n",
            "          [0.0000e+00, 1.1993e+00, 1.7588e+00, 1.8410e+00]],\n",
            "\n",
            "         [[1.7805e+00, 2.4775e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [2.2297e-01, 2.7616e+00, 7.4701e-01, 1.9262e+00],\n",
            "          [2.4662e+00, 4.9333e+00, 9.9655e-01, 2.0489e+00],\n",
            "          [0.0000e+00, 2.9159e+00, 1.4897e+00, 2.0918e+00]]],\n",
            "\n",
            "\n",
            "        [[[9.6451e-01, 7.0249e-01, 1.1518e+00, 0.0000e+00],\n",
            "          [1.5187e-01, 7.7739e-01, 6.8306e-01, 6.7169e-01],\n",
            "          [0.0000e+00, 3.3754e-02, 4.9684e-01, 6.9931e-01],\n",
            "          [0.0000e+00, 2.0648e-01, 4.5686e-01, 1.1336e+00]],\n",
            "\n",
            "         [[0.0000e+00, 9.0189e-02, 1.3918e-01, 0.0000e+00],\n",
            "          [0.0000e+00, 3.5929e-01, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 1.6733e+00, 6.4868e-01, 5.9747e-01],\n",
            "          [4.6441e-01, 1.3490e+00, 7.4098e-01, 1.8731e-01],\n",
            "          [1.1529e+00, 2.9113e+00, 2.4592e+00, 1.9008e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 1.4708e-01, 3.8720e-01],\n",
            "          [1.0365e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8461e-01],\n",
            "          [3.7660e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 1.6530e-01, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 5.5463e-01, 0.0000e+00],\n",
            "          [0.0000e+00, 3.0737e-01, 1.5376e-02, 0.0000e+00]],\n",
            "\n",
            "         [[1.6070e+00, 1.2180e-01, 0.0000e+00, 0.0000e+00],\n",
            "          [7.6583e-01, 4.2638e-01, 2.8100e-01, 0.0000e+00],\n",
            "          [1.9061e+00, 1.5422e+00, 5.8904e-01, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 5.7971e-01, 0.0000e+00]]]], device='cuda:0',\n",
            "       grad_fn=<ReluBackward1>), tensor([[ 0.0624, -0.0787]], device='cuda:0', grad_fn=<FusedDropoutBackward>))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "model = Model(2).cuda()\n",
        "a = model(torch.from_numpy(np.empty((1,20,3,112,112))).type(torch.cuda.FloatTensor))\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJGGG1JpFtmk"
      },
      "outputs": [],
      "source": [
        "class Average(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        \n",
        "def calculate_accuracy(outputs, targets):\n",
        "    batch_size = targets.size(0)\n",
        "\n",
        "    _, pred = outputs.topk(1, 1, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(targets.view(1, -1))\n",
        "    n_correct_elems = correct.float().sum().item()\n",
        "    return 100* n_correct_elems / batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3Kii5xaZU9H",
        "outputId": "bcf977a6-5613-470f-c360-3cb81cfe5b34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 19/20] [Batch 16 / 17] [Loss: 0.314350, Acc: 91.18%]"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "def train(epoch, num_epochs, data_loader, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    losses = Average()\n",
        "    accuracies = Average()\n",
        "    t = []\n",
        "    for i, (inputs, targets) in enumerate(data_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            targets = targets.type(torch.cuda.LongTensor)\n",
        "            inputs = inputs.cuda()\n",
        "        _,outputs = model(inputs)\n",
        "        loss  = criterion(outputs,targets.type(torch.cuda.LongTensor))\n",
        "        acc = calculate_accuracy(outputs, targets.type(torch.cuda.LongTensor))\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "        accuracies.update(acc, inputs.size(0))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        sys.stdout.write(\n",
        "                \"\\r[Epoch %d/%d] [Batch %d / %d] [Loss: %f, Acc: %.2f%%]\"\n",
        "                % (\n",
        "                    epoch,\n",
        "                    num_epochs,\n",
        "                    i,\n",
        "                    len(data_loader),\n",
        "                    losses.avg,\n",
        "                    accuracies.avg))\n",
        "    torch.save(model.state_dict(),'/content/checkpoint.pt')\n",
        "    return losses.avg,accuracies.avg\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= 1e-5 ,weight_decay = 1e-5)\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "trainingloss = []\n",
        "trainingaccuracy = []\n",
        "for epoch in range(20):\n",
        "    l, acc = train(epoch,20,t_loader,model,criterion,optimizer)\n",
        "    trainingloss.append(l)\n",
        "    trainingaccuracy.append(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmYzcmwEM8He",
        "outputId": "7ed8f417-9f18-4177-cd91-fd4e6eeb2ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Batch 4 / 5]  [Loss: 1.875634, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.835700, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.813352, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.814552, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.868422, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.848702, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.875714, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.871994, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.859832, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.871200, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.862528, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.845639, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.840166, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.825175, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.850135, Acc: 35.29%]\n",
            "Accuracy 35.294117647058826\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.830326, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.824729, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.855898, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.839242, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n",
            "Testing\n",
            "[Batch 4 / 5]  [Loss: 1.844360, Acc: 29.41%]\n",
            "Accuracy 29.41176470588235\n"
          ]
        }
      ],
      "source": [
        "def test(epoch,model, data_loader ,criterion):\n",
        "    print('Testing')\n",
        "    model.eval()\n",
        "    losses = Average()\n",
        "    accuracies = Average()\n",
        "    pred = []\n",
        "    true = []\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(data_loader):\n",
        "            if torch.cuda.is_available():\n",
        "                targets = targets.cuda().type(torch.cuda.FloatTensor)\n",
        "                inputs = inputs.cuda()\n",
        "            _,outputs = model(inputs)\n",
        "            loss = torch.mean(criterion(outputs, targets.type(torch.cuda.LongTensor)))\n",
        "            acc = calculate_accuracy(outputs,targets.type(torch.cuda.LongTensor))\n",
        "            _,p = torch.max(outputs,1) \n",
        "            true += (targets.type(torch.cuda.LongTensor)).detach().cpu().numpy().reshape(len(targets)).tolist()\n",
        "            pred += p.detach().cpu().numpy().reshape(len(p)).tolist()\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "            accuracies.update(acc, inputs.size(0))\n",
        "            sys.stdout.write(\n",
        "                    \"\\r[Batch %d / %d]  [Loss: %f, Acc: %.2f%%]\"\n",
        "                    % (\n",
        "                        i,\n",
        "                        len(data_loader),\n",
        "                        losses.avg,\n",
        "                        accuracies.avg\n",
        "                        )\n",
        "                    )\n",
        "        print('\\nAccuracy {}'.format(accuracies.avg))\n",
        "    return true,pred,losses.avg,accuracies.avg\n",
        "\n",
        "tesingloss = []\n",
        "testingacc = []\n",
        "for epoch in range(20):\n",
        "    true,pred,tl,t_acc = test(epoch,model,v_loader,criterion)\n",
        "    tesingloss.append(tl)\n",
        "    testingacc.append(t_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJjCR0-7NzDY",
        "outputId": "b22818bf-d39f-44b3-ebe2-e9cf86f83793"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-662cf7ac3dd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#print(confusion_matrix(true,pred))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "def accuracy(confusion_matrix):\n",
        "  diagnol = confusion_matrix.trace()\n",
        "  sum = confusion_matrix.sum()\n",
        "  return diagnol/sum\n",
        "\n",
        "\n",
        "print(confusion_matrix(true,pred))\n",
        "print(\"Accuracy: \", )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DeepFakeDetection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}